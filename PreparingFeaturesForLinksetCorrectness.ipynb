{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning linkset correctness - (MTSR 2019)\n",
    "\n",
    "**Research Questions**: \n",
    "- can we train a neural network distingushing between correct and uncorrect links?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training sets\n",
    "The training set is based on a set of linksets that have been generated building [Linked Thesaurus fRamework for Environment (LusTRE)](http://linkeddata.ge.imati.cnr.it/) as part of the research activity carried out during two EU funded projects: NatureSDIPlus and eENVplus. \n",
    "\n",
    "\n",
    "The procedure adopted to prepare a view of the linksets with the label, BT,NT,RT are described in \n",
    "* [Preparing Linkset involving local dumps](http://localhost:8888/notebooks/ai-related/LinkCorrectess/PreparingLinksetWithLocalDumps.ipynb)\n",
    ", which include all the linksets not involving DBPEDIA\n",
    "* [Preparing Linkset involving Dbpedia](http://localhost:8888/notebooks/ai-related/LinkCorrectess/PreparingLinksetWithDBPEDIA.ipynb)\n",
    "\n",
    "### Useful tutorial \n",
    " -  A  useful tutorial about pandas's dataframe is available at https://data36.com/pandas-tutorial-1-basics-reading-data-files-dataframes-data-selection/\n",
    " - creating and editing https://www.shanelynn.ie/using-pandas-dataframe-creating-editing-viewing-data-in-python/\n",
    " - [Advanced Jupyter Notebook Tricks — Part I](https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global variables \n",
    "path=\"data/\" # path where to find data\n",
    "#namesa=['sBT','sprefLabel','sURI','oURI','oprefLabel','oBT', 'KindOfLink'] #column names for training data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What features are we going to consider to characterize a link?\n",
    "\n",
    "\n",
    "Text and Conceptual similarity among prefered labels  and  broader terms are considered as significant features on which classify a link.\n",
    "\n",
    "Different approaches are available in order to work out the text similarity\n",
    "\n",
    "### word2Vec\n",
    "- A pretrained model for text similarity http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/ (**pretrained model in Users/bubu/model/ but instructions outdated** )\n",
    "- example of usage in  https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "Others  resources \n",
    "- https://medium.freecodecamp.org/how-to-get-started-with-word2vec-and-then-how-to-make-it-work-d0a2fca9dad3\n",
    "- https://www.slideshare.net/lechatpito\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- [Vector Representations of Words IN TF](https://www.tensorflow.org/tutorials/representation/word2vec)\n",
    "- [Stanford courser - Word Vector Representations: word2vec](https://www.youtube.com/watch?v=ERibwqs9p38)\n",
    "- using word2Vec in rapidMiner https://community.rapidminer.com/discussion/43860/synonym-detection-with-word2vec\n",
    " -https://www.neuralmarkettrends.com/word2vec-example-process-rapidminer\n",
    "\n",
    "### Glo Ve\n",
    "\n",
    "- https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b\n",
    "\n",
    "### Text Similarity\n",
    "- Basic text similarities https://pypi.org/project/textdistance/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Attempt 1: Let's initialize the Word2Vec with a pre-existing model\n",
    "\n",
    "## Design choices\n",
    "- **design choice 1**: We use the Google’s pre-trained model see [here](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/). It’s 1.5GB! It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features.\n",
    "- **design choice 2**: Similarity(s1,s2) implements a first attempt to work out the similarity between two set of words. It works out the max of sim on the pairs taken in the cardinal product of the sets, not considering the stoplist. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes very long to be executed\n",
    "# https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "#word_vectors = model.wv\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"/Users/bubu/model/GoogleNews-vectors-negative300.bin\", binary=True)  # C bin format\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1 - How to call the similarity between vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675937254097414\n"
     ]
    }
   ],
   "source": [
    "#similarity = word_vectors.similarity('africa'.lower(), 'Countries in Africa'.lower().split())\n",
    "#print(similarity)\n",
    "docdistance=word_vectors.wmdistance('africa', 'Africa')\n",
    "print(docdistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v= ['woman','man', 'house', 'pippo']\n",
    "def printifvector(v):\n",
    "    for e in v:\n",
    "        print(e +\":\")\n",
    "        try:\n",
    "        #vector = word_vectors.wv.word_vec( word_vectors.doesnt_match(e), use_norm=True)\n",
    "            print(word_vectors.get_vector(e)) \n",
    "        except KeyError as ex:\n",
    "            print('exception for ' +e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 - Procedure  work to out similarity on BT according to the first attempt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to work out similarity between two set of words\n",
    "#lw1 and lw2 are two documents containing set of words\n",
    "def similarityBetweenSetsSplitingInWords(s1,s2):\n",
    "    ## remove common words and not indexed words and tokenize\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    if (type(s1) is not str) or (type(s2) is not str):\n",
    "        return 0.0\n",
    "    # tokenize and removing |\n",
    "    remove= lambda x :x.replace('|',\"\")\n",
    "    lw1=list(map(remove, s1.lower().split()))\n",
    "    lw2=list(map(remove,s2.lower().split()))\n",
    "    \n",
    "    strip= lambda x :x.strip()\n",
    "    lw1=list(map(strip, lw1))\n",
    "    lw2=list(map(strip, lw2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## what words are indexed?\n",
    "    lw1 =  [word for word in lw1 if word not in stoplist]\n",
    "    lw2 =  [word for word in lw2 if word not in stoplist]\n",
    "    \n",
    "    print(lw1)\n",
    "    print(lw2)\n",
    "    amax=-1.0\n",
    "    \n",
    "    # if one of the sets is empty it returns 0\n",
    "    if not ((len(lw1) == 0) or (len(lw2) == 0)): \n",
    "        for i in lw1 :  \n",
    "            lmax=-1.0\n",
    "            try:\n",
    "                #test if i is indexed otherwise exception\n",
    "                word_vectors.get_vector(i) \n",
    "            except KeyError as ex:\n",
    "                print('exception for ' +i)\n",
    "                continue\n",
    "            for ii in lw2 :\n",
    "                try:\n",
    "                     #test if i is indexed otherwise exception\n",
    "                    word_vectors.get_vector(ii)\n",
    "                except KeyError as ex:\n",
    "                    print('exception for ' +ii)\n",
    "                    continue\n",
    "                sim=word_vectors.similarity(i,ii)\n",
    "                #print('sim(%s, %s) = %f' %(i,ii,sim) )   \n",
    "                lmax = max(sim , lmax)\n",
    "                amax+=lmax\n",
    "                       \n",
    "    return amax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['africa']\n",
      "['africa']\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(similarityBetweenSetsSplitingInWords(\"africa\", 'africa'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maxInSplitWords (M)\n",
    "Given \n",
    "*  two sets of words $S_1$ and $S_2$\n",
    "*  a similarity functions\n",
    "*  $(x_i,y_j) \\in S_1 \\times S_2$\n",
    "  \n",
    "maxInSplitWords implements the following mathematical function\n",
    "\n",
    "$\\text{maxInSplitWords(x,y,sim)}=\\text{MAX}_{i,j}(sim(x_i,y_j))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to work out similarity between two set of words\n",
    "# lw1 and lw2 are two documents containing set of words\n",
    "# function is the similarity function to apply\n",
    "# it returns the maximun similarity comaring the set product\n",
    "def maxInSplitWords(s1,s2, function):\n",
    "    ## remove common words and not indexed words and tokenize\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    if (type(s1) is not str) or (type(s2) is not str):\n",
    "        return 0.0\n",
    "    # tokenize and removing |\n",
    "    remove= lambda x :x.replace('|',\"\")\n",
    "    lw1=list(map(remove, s1.lower().split()))\n",
    "    lw2=list(map(remove,s2.lower().split()))\n",
    "    \n",
    "    strip= lambda x :x.strip()\n",
    "    lw1=list(map(strip, lw1))\n",
    "    lw2=list(map(strip, lw2))\n",
    "    \n",
    "    \n",
    "    ## what words are indexed?\n",
    "    lw1 =  [word for word in lw1 if word not in stoplist]\n",
    "    lw2 =  [word for word in lw2 if word not in stoplist]\n",
    "    \n",
    "    print(lw1)\n",
    "    print(lw2)\n",
    "    lmax=float('nan')\n",
    "    firstTime = True\n",
    "    # if one of the sets is empty it returns 0\n",
    "    if not ((len(lw1) == 0) or (len(lw2) == 0)): \n",
    "        for i in lw1 :  \n",
    "            for ii in lw2 :\n",
    "                sim=function(i,ii)\n",
    "                if ( math.isnan(lmax)) :\n",
    "                    lmax=sim\n",
    "                else:\n",
    "                    lmax = max(sim , lmax)                     \n",
    "    return lmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummingMax (SM)\n",
    "\n",
    "Given \n",
    "*  two sets of words $S_1$ and $S_2$\n",
    "*  a similarity functions\n",
    "*  $(x_i,y_j) \\in S_1 \\times S_2$\n",
    "  \n",
    "summingMax implements the following mathematical function\n",
    "\n",
    "\n",
    "$\\text{summingMax(x,y,sim)}=\\sum_i{\\text{MAX}_{i,j}(sim(x_i,y_j))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to work out similarity between two set of words\n",
    "# lw1 and lw2 are two documents containing set of words\n",
    "# function is the similarity function to apply\n",
    "import math\n",
    "def summingMax(s1, s2, function):\n",
    "    ## remove common words and not indexed words and tokenize\n",
    "    stoplist = set('for a of the and to in'.split())\n",
    "    if (type(s1) is not str) or (type(s2) is not str):\n",
    "        return 0.0\n",
    "    # tokenize and removing |\n",
    "    remove= lambda x :x.replace('|',\"\")\n",
    "    lw1=list(map(remove, s1.lower().split()))\n",
    "    lw2=list(map(remove,s2.lower().split()))\n",
    "    \n",
    "    strip= lambda x :x.strip()\n",
    "    lw1=list(map(strip, lw1))\n",
    "    lw2=list(map(strip, lw2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## what words are indexed?\n",
    "    lw1 =  [word for word in lw1 if word not in stoplist]\n",
    "    lw2 =  [word for word in lw2 if word not in stoplist]\n",
    "    \n",
    "    print(lw1)\n",
    "    print(lw2)\n",
    "   \n",
    "    amax=float('nan')\n",
    "    #firstTime = True\n",
    "    # if one of the sets is empty it returns 0\n",
    "    if not ((len(lw1) == 0) or (len(lw2) == 0)): \n",
    "        for i in lw1 :  \n",
    "            lmax=float('nan')\n",
    "            for ii in lw2 :\n",
    "                sim=function(i,ii)\n",
    "                if (math.isnan(lmax)) :\n",
    "                    lmax=sim\n",
    "                    #firstTime =False\n",
    "                else:\n",
    "                    lmax = max(sim , lmax)\n",
    "            if (math.isnan(amax)):\n",
    "                amax=lmax;\n",
    "            else: \n",
    "                amax+=lmax;\n",
    "    return amax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(summingMax('', '', textdistance.hamming.normalized_similarity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the List of validated linksets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thist2AGROVOCEnrichedLinkeset.csv\n",
      "Thist2EUROVOCEnrichedLinkeset.csv\n",
      "ThIST2BpediaEnrichedLinkeset.csv\n",
      "Thist2GEMETEnrichedLinkeset.csv\n"
     ]
    }
   ],
   "source": [
    "# https://dzone.com/articles/listing-a-directory-with-python\n",
    "import os\n",
    "\n",
    "TrainingFiles= filter(lambda x: x.endswith('EnrichedLinkeset.csv'),  os.listdir(path)) \n",
    " \n",
    "for file in TrainingFiles : \n",
    "    print(file)\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def workOutSim():\n",
    "    TrainingFiles= filter(lambda x: x.endswith('EnrichedLinkeset.csv'),  os.listdir(path)) \n",
    "    for file in TrainingFiles: \n",
    "        print(file)\n",
    "        lf=pd.read_csv(path+ file,delimiter=\",\")\n",
    "        lf=lf.fillna('')\n",
    "        \n",
    "        #BT  \n",
    "        lf['BT_similaritySInW']=0.0 \n",
    "        lf['BT_wmdistance']=lf['BT_Mwmdistance']= lf['BT_SMwmdistance']= 0.0\n",
    "        lf['BT_nhammingSim']= lf['BT_MnhammingSim']= lf['BT_SMnhammingSim']= 0.0\n",
    "        #Preferred Label\n",
    "        lf['PrefLabel_similaritySInW']=0.0 \n",
    "        lf['PrefLabel_wmdistance']=lf['PrefLabel_Mwmdistance']= lf['PrefLabel_SMwmdistance']= 0.0\n",
    "        lf['PrefLabel_nhammingSim']= lf['PrefLabel_MnhammingSim']= lf['PrefLabel_SMnhammingSim']= 0.0\n",
    "         #NT\n",
    "        lf['RT_similaritySInW']=0.0 \n",
    "        lf['RT_wmdistance']=lf['RT_Mwmdistance']= lf['RT_SMwmdistance']= 0.0\n",
    "        lf['RT_nhammingSim']= lf['RT_MnhammingSim']= lf['RT_SMnhammingSim']= 0.0\n",
    "\n",
    "\n",
    "        l = range(1, len(lf))\n",
    "        for i in l:\n",
    "            if (lf.sBT.iloc[i]!='') and (lf.oBT.iloc[i]!=''):\n",
    "                lf['BT_similaritySInW'][i] =similarityBetweenSetsSplitingInWords(lf.sBT[i], lf.oBT[i])\n",
    "                lf['BT_wmdistance'][i]=word_vectors.wmdistance(lf.sBT[i], lf.oBT[i])\n",
    "                lf['BT_Mwmdistance'][i]= maxInSplitWords(lf.sBT[i], lf.oBT[i], word_vectors.wmdistance)\n",
    "                lf['BT_SMwmdistance'][i]= summingMax(lf.sBT[i], lf.oBT[i], word_vectors.wmdistance)\n",
    "                lf['BT_nhammingSim'][i]=textdistance.hamming.normalized_similarity(lf.sBT[i], lf.oBT[i])\n",
    "                lf['BT_MnhammingSim'][i]=maxInSplitWords(lf.sBT[i], lf.oBT[i],textdistance.hamming.normalized_similarity)\n",
    "                lf['BT_SMnhammingSim'][i] =summingMax(lf.sBT[i], lf.oBT[i],textdistance.hamming.normalized_similarity)\n",
    "\n",
    "            if (lf.sRT.iloc[i]!='') and (lf.oRT.iloc[i]!=''):\n",
    "                 lf['RT_similaritySInW'][i] =similarityBetweenSetsSplitingInWords(lf.sRT[i], lf.oRT[i])\n",
    "                 lf['RT_wmdistance'][i]=word_vectors.wmdistance(lf.sRT[i], lf.oRT[i])\n",
    "                 lf['RT_Mwmdistance'][i]= maxInSplitWords(lf.sRT[i], lf.oRT[i], word_vectors.wmdistance)\n",
    "                 lf['RT_SMwmdistance'][i]= summingMax(lf.sRT[i], lf.oRT[i], word_vectors.wmdistance)\n",
    "                 lf['RT_nhammingSim'][i]=textdistance.hamming.normalized_similarity(lf.sRT[i], lf.oRT[i])\n",
    "                 lf['RT_MnhammingSim'][i]=maxInSplitWords(lf.sRT[i], lf.oRT[i],textdistance.hamming.normalized_similarity)\n",
    "                 lf['RT_SMnhammingSim'][i] =summingMax(lf.sRT[i], lf.oRT[i],textdistance.hamming.normalized_similarity)\n",
    "                \n",
    "            if (lf.sPrefLabel.iloc[i]!='') and (lf.oBT.iloc[i]!=''):\n",
    "                lf['PrefLabel_similaritySInW'][i] =similarityBetweenSetsSplitingInWords(lf.sPrefLabel[i], lf.oPrefLabel[i])\n",
    "                lf['PrefLabel_wmdistance'][i]=word_vectors.wmdistance(lf.sPrefLabel[i], lf.oPrefLabel[i])\n",
    "                lf['PrefLabel_Mwmdistance'][i]= maxInSplitWords(lf.sPrefLabel[i], lf.oPrefLabel[i], word_vectors.wmdistance)\n",
    "                lf['PrefLabel_SMwmdistance'][i]= summingMax(lf.sPrefLabel[i], lf.oPrefLabel[i], word_vectors.wmdistance)\n",
    "                lf['PrefLabel_nhammingSim'][i]=textdistance.hamming.normalized_similarity(lf.sPrefLabel[i], lf.oPrefLabel[i])\n",
    "                lf['PrefLabel_MnhammingSim'][i]=maxInSplitWords(lf.sPrefLabel[i], lf.oPrefLabel[i],textdistance.hamming.normalized_similarity)\n",
    "                lf['PrefLabel_SMnhammingSim'][i] =summingMax(lf.sPrefLabel[i], lf.oPrefLabel[i],textdistance.hamming.normalized_similarity)\n",
    "            \n",
    "        lf.to_csv(path+file.replace('.csv','SimilarityResult.csv'), sep=';')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workOutSim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning   the Similarity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThIST2BpediaEnrichedLinkesetSimilarityResult.csv\n",
      "Thist2AGROVOCEnrichedLinkesetSimilarityResult.csv\n",
      "Thist2GEMETEnrichedLinkesetSimilarityResult.csv\n",
      "Thist2EUROVOCEnrichedLinkesetSimilarityResult.csv\n"
     ]
    }
   ],
   "source": [
    "TrainingFiles= filter(lambda x: x.endswith('SimilarityResult.csv'),  os.listdir(path)) \n",
    "for file in TrainingFiles: \n",
    "    print(file)\n",
    "    lf=pd.read_csv(path+ file,delimiter=\";\")\n",
    "    lf.to_csv(path+file.replace('SimilarityResult.csv','SimilarityResultOld.csv'), sep=';')\n",
    "    lf1=lf[['s','o','KindOfLink','sBT','sNT','sRT','sPrefLabel','sAltLabels','oPrefLabel','oAltLabels','oBT','oNT','oRT','RT_similaritySInW','RT_wmdistance', 'RT_Mwmdistance', 'RT_SMwmdistance','RT_nhammingSim','RT_MnhammingSim','RT_SMnhammingSim','BT_similaritySInW','BT_wmdistance','BT_Mwmdistance','BT_SMwmdistance','BT_nhammingSim','BT_MnhammingSim','BT_SMnhammingSim','PrefLabel_similaritySInW','PrefLabel_wmdistance','PrefLabel_Mwmdistance','PrefLabel_SMwmdistance','PrefLabel_nhammingSim','PrefLabel_MnhammingSim','PrefLabel_SMnhammingSim'\n",
    "]]\n",
    "    lf1.to_csv(path+file.replace('SimilarityResult.csv','SimilarityResult1.csv'), sep=';')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
